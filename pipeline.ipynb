{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frmIpPkOglXn"
      },
      "outputs": [],
      "source": [
        "# Installing the necessary libraries\n",
        "!pip install numpy scipy networkx matplotlib powerlaw networkit python-louvain scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "E4W4Uj78gveY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8be905-3d71-4376-94f5-f1548e20d6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import sklearn\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from collections import deque\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import mannwhitneyu\n",
        "from scipy.stats import ranksums\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#----------------------------FUNCTIONS------------------------------------------\n",
        "\n",
        "# Degree Centrality (degree of a node) calculations\n",
        "\n",
        "def degrees(adjacency_matrix):\n",
        "  DC_values = np.sum(adjacency_matrix, axis=1)\n",
        "  return DC_values\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Betweenness Centrality calculations (via Ulrik Brandes algorithm)\n",
        "\n",
        "  # OBS: Outputs both the vertex centrality and the edge betweenness centrality\n",
        "\n",
        "def betweenness_centrality(adjacency_matrix):\n",
        "\n",
        "  size = adjacency_matrix.shape[0]\n",
        "  A = adjacency_matrix.copy()\n",
        "  Q = deque()\n",
        "  S = []\n",
        "  BC_values = [0]*size\n",
        "  EBC_values = {(i, j): 0.0 for i in range(size) for j in range(size)}\n",
        "\n",
        "\n",
        "  for s in range(size):\n",
        "\n",
        "    #---------------------SSSP Problem----------------------------\n",
        "\n",
        "    # Create a list of lists with the predecessors of each vertex\n",
        "    P = [[] for _ in range(size)]\n",
        "    # Create a list with the distances of shortest paths from node s to t\n",
        "    d = [-1]*size\n",
        "    d[s] = 0\n",
        "    # Create a list with the numbers of shortest paths from node s to t\n",
        "    sigma = [0]*size\n",
        "    sigma[s] = 1\n",
        "    Q.append(s)\n",
        "\n",
        "    while Q:\n",
        "      v = Q.popleft()\n",
        "      S.append(v)\n",
        "\n",
        "      neighbours = np.nonzero(A[v])[0]\n",
        "\n",
        "      for w in neighbours:\n",
        "        # w found for the first time?\n",
        "        if (d[w] < 0) :\n",
        "          d[w] = d[v] + 1\n",
        "          Q.append(w)\n",
        "        # Shortest path from w via v?\n",
        "        if (d[w] == d[v] + 1) :\n",
        "          sigma[w] = sigma[w] + sigma[v]\n",
        "          P[w].append(v)\n",
        "\n",
        "    #-------------------------Accumulation--------------------------------------\n",
        "\n",
        "    # Create a list of dependencies from s including node v\n",
        "    delta = [0]*size\n",
        "    while S:\n",
        "      w = S.pop()\n",
        "      for v in P[w]:\n",
        "        c = (sigma[v]/sigma[w])*(1+delta[w])\n",
        "        EBC_values[(v,w)] = EBC_values[(v,w)] + c\n",
        "        delta[v] = delta[v] + c\n",
        "        if w != s:\n",
        "          BC_values[w] = BC_values[w] + delta[w]\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "\n",
        "  return BC_values, EBC_values\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# Nodal Clustering Coefficient calculations\n",
        "\n",
        "def nodal_clustering_coefficients(adjacency_matrix):\n",
        "\n",
        "  size = adjacency_matrix.shape[0]\n",
        "  local_clustering_coefficients = []\n",
        "\n",
        "  for i in range(size):\n",
        "    neighbours = np.nonzero(adjacency_matrix[i])[0]\n",
        "    neighbourhood_size = len(neighbours)\n",
        "\n",
        "    # Exception for a node that has degree 1\n",
        "    if (neighbourhood_size < 2):\n",
        "      frac = 0\n",
        "      local_clustering_coefficients.append(frac)\n",
        "      continue\n",
        "\n",
        "    numerator = 0\n",
        "    denominator = (neighbourhood_size*(neighbourhood_size-1))/2\n",
        "\n",
        "    for node in neighbours:\n",
        "      for other_node in neighbours:\n",
        "        if (node < other_node) and (adjacency_matrix[node][other_node] == 1):\n",
        "          numerator+=1\n",
        "\n",
        "    frac = numerator/denominator\n",
        "\n",
        "    local_clustering_coefficients.append(frac)\n",
        "\n",
        "  return local_clustering_coefficients\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Eigenvector Centrality calculation (via power iteration method)\n",
        "\n",
        "def eigenvector_centrality(adjacency_matrix, max_iter=1000, tol=1e-6):\n",
        "\n",
        "    # Initialize the eigenvector centrality vector\n",
        "    n = adjacency_matrix.shape[0]\n",
        "    centrality = np.ones(n)\n",
        "\n",
        "    # Power iteration method to find the principal eigenvector\n",
        "    for i in range(max_iter):\n",
        "        centrality_next = np.dot(adjacency_matrix, centrality)\n",
        "\n",
        "        # Normalize the eigenvector\n",
        "        centrality_next /= np.linalg.norm(centrality_next, 2)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(centrality_next - centrality, 2) < tol:\n",
        "            break\n",
        "\n",
        "        centrality = centrality_next\n",
        "\n",
        "    return centrality\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Function to calculate metrics for an adjacency matrix\n",
        "\n",
        "def calculate_metrics(adjacency_matrix):\n",
        "\n",
        "    # Calculate the entropy of the desired metrics (using the minimum)\n",
        "    DC = degrees(adjacency_matrix)\n",
        "    DC = np.min(DC)\n",
        "\n",
        "    BC = betweenness_centrality(adjacency_matrix)[0]\n",
        "    BC = np.min(BC)\n",
        "\n",
        "    CC = nodal_clustering_coefficients(adjacency_matrix)\n",
        "    CC = np.min(CC)\n",
        "\n",
        "    EC = eigenvector_centrality(adjacency_matrix)\n",
        "    EC = np.min(EC)\n",
        "\n",
        "    return {\n",
        "        'min_DC': DC,\n",
        "        'min_BC': BC,\n",
        "        'min_CC': CC,\n",
        "        'min_EC': EC\n",
        "    }\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "-DertqOCgxfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Pipeline\n",
        "\n",
        "# Define the dataset directory path\n",
        "main_directory = '/content/drive/My Drive/ABIDE/abide'\n",
        "\n",
        "# List all folders in the main directory\n",
        "folders_list = os.listdir(main_directory)\n",
        "\n",
        "# Create a list containing all folder\n",
        "folders = []\n",
        "for folder in folders_list:\n",
        "  folders.append(folder)\n",
        "\n",
        "folders = sorted(folders)\n",
        "folders.remove('.DS_Store')\n",
        "\n",
        "# Initialize lists to store data for CSV\n",
        "patient_names = []\n",
        "class_labels = []\n",
        "\n",
        "# Initialize the metrics_data dictionary outside the loop\n",
        "metric_keys = ['min_DC','min_BC','min_CC','min_EC']\n",
        "metrics_data = {metric_name: [] for metric_name in metric_keys}\n",
        "\n",
        "# Iterate through folders\n",
        "i=0\n",
        "for f in folders:\n",
        "  print(i+1)\n",
        "\n",
        "  file_path = main_directory + '/' + f + '/' + f + '_harvard48_correlation_matrix.mat'\n",
        "\n",
        "  # Load the MAT file\n",
        "  mat_file = scipy.io.loadmat(file_path)\n",
        "\n",
        "  # Accessing data\n",
        "  data = mat_file.get('data')\n",
        "\n",
        "  # Exclude diagonal entries (self-loops)\n",
        "  np.fill_diagonal(data, 0)\n",
        "\n",
        "  # Take the absolute value of the correlations\n",
        "  abs_correlations = np.abs(data)\n",
        "\n",
        "  # Flatten the matrix into 1D array\n",
        "  flattened_correlations = abs_correlations.flatten()\n",
        "\n",
        "  # Find the 80th percentile\n",
        "  percentile_80 = np.percentile(flattened_correlations, 80)\n",
        "\n",
        "  # Binarize the matrix based on the 80th percentile\n",
        "  binary_matrix = (abs_correlations >= percentile_80).astype(int)\n",
        "\n",
        "  # Create a numpy array containing the adjacency matrix\n",
        "  A = binary_matrix\n",
        "\n",
        "  # Identify the name of the patient and the class\n",
        "  patient_name = f\n",
        "\n",
        "  if \"control\" in patient_name:\n",
        "    class_label = \"control\"\n",
        "  elif \"patient\" in patient_name:\n",
        "    class_label = \"patient\"\n",
        "\n",
        "  # Append the current name of the patient to the list\n",
        "  patient_names.append(patient_name)\n",
        "\n",
        "  # Append the current class label to the list\n",
        "  class_labels.append(class_label)\n",
        "\n",
        "  # Calculate metrics for the adjacency matrix\n",
        "  metrics = calculate_metrics(A)\n",
        "  # metrics_data = {metric_name: [] for metric_name in metrics.keys()}\n",
        "\n",
        "  # Append data to lists\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    metrics_data[metric_name].append(metric_value)\n",
        "\n",
        "  i+=1\n",
        "\n",
        "print(\"Length of patient_names:\", len(patient_names))\n",
        "print(\"Length of class_labels:\", len(class_labels))\n",
        "for metric_name, metric_values in metrics_data.items():\n",
        "    print(f\"Length of {metric_name}:\", len(metric_values))\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {'Patient': patient_names, 'Class': class_labels, **metrics_data}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "google_drive_path = '/content/drive/My Drive/ABIDE/min_80th.csv'\n",
        "df.to_csv(google_drive_path, index=False)"
      ],
      "metadata": {
        "id": "Cx9mECK3iPQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical analysis\n",
        "\n",
        "# Read the .csv file\n",
        "google_drive_path = '/content/drive/My Drive/ABIDE/min_80th.csv'\n",
        "df = pd.read_csv(google_drive_path)\n",
        "\n",
        "# Descriptive statistics\n",
        "\n",
        "df_describe = df.groupby('Class').describe()\n",
        "df_describe = df_describe.unstack().unstack(1)\n",
        "\n",
        "# Save descriptive statistics to CSV\n",
        "#df_describe.to_csv('/content/drive/My Drive/ABIDE/min_80th_t_test_descriptive_statistics.csv')\n",
        "\n",
        "# Plot distributions for each attribute separated by class\n",
        "for column in df.columns[2:]:  # Exclude the 'Class' column\n",
        "    percentile = '80'\n",
        "\n",
        "    # Create a line plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Use hue parameter to specify the grouping variable\n",
        "    if column == 'min_DC':\n",
        "      sns.histplot(data=df, x=column, hue='Class', kde=False, binwidth=1)\n",
        "    else:\n",
        "      sns.histplot(data=df, x=column, hue='Class', kde=False)\n",
        "\n",
        "    plt.title(f'Distribution of {column} by Class for ' + percentile + 'th percentile')\n",
        "\n",
        "    # Save the figure\n",
        "    file_label = column\n",
        "    img_name = percentile + 'th_' + file_label + '_plot'\n",
        "    plt.savefig('/content/drive/My Drive/ABIDE/' + img_name + '.png')\n",
        "\n",
        "    # Clear the current figure\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "# Set the column labels\n",
        "name_column = 'Patient'\n",
        "class_column = 'Class'\n",
        "\n",
        "# Get a list of all attribute columns\n",
        "attribute_columns = df.columns[(df.columns != class_column) & (df.columns != name_column)]\n",
        "\n",
        "# Convert data types to numeric\n",
        "df[attribute_columns] = df[attribute_columns].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Perform t-test for each attribute\n",
        "for attribute_column in attribute_columns:\n",
        "\n",
        "    control_data = df[df[class_column] == 'control'][attribute_column]\n",
        "    patient_data = df[df[class_column] == 'patient'][attribute_column]\n",
        "\n",
        "    t_stat, p_value = ttest_ind(control_data, patient_data)\n",
        "\n",
        "\n",
        "    # Print the results for each attribute\n",
        "    #print(f'T-test for {attribute_column}: T-statistic={t_stat}, p-value={p_value}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save t-test results to CSV\n",
        "t_test_results = []\n",
        "\n",
        "for attribute_column in attribute_columns:\n",
        "    control_data = df[df[class_column] == 'control'][attribute_column]\n",
        "    patient_data = df[df[class_column] == 'patient'][attribute_column]\n",
        "\n",
        "    t_stat, p_value = ttest_ind(control_data, patient_data)\n",
        "\n",
        "    # Append results to the list\n",
        "    t_test_results.append({\n",
        "        'Attribute': attribute_column,\n",
        "        'T-statistic': t_stat,\n",
        "        'P-value': p_value\n",
        "    })\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df_t_test_results = pd.DataFrame(t_test_results)\n",
        "print(df_t_test_results)\n",
        "\n",
        "# Save t-test results to CSV\n",
        "df_t_test_results.to_csv('/content/drive/My Drive/ABIDE/min_80th_t_test_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "h2O8AG8Ciian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical analysis part 2 (what if the distributions are not gaussian?)\n",
        "\n",
        "# Initialize an empty list for results\n",
        "mann_whitney_results = []\n",
        "\n",
        "label_list = ['min_DC', 'min_BC', 'min_CC', 'min_EC']\n",
        "\n",
        "for L in label_list:\n",
        "\n",
        "    list_control = []\n",
        "    list_patient = []\n",
        "\n",
        "    # Iterate over rows\n",
        "    for index, row in df.iterrows():\n",
        "        if row['Class'] == 'control':\n",
        "            list_control.append(row[L])\n",
        "        elif row['Class'] == 'patient':\n",
        "            list_patient.append(row[L])\n",
        "\n",
        "    # Perform Mann-Whitney U test\n",
        "    statistic, p_value = mannwhitneyu(list_control, list_patient)\n",
        "\n",
        "    # Interpret the results\n",
        "    significance = \"Significant\" if p_value < 0.05 else \"Not Significant\"\n",
        "\n",
        "    # Append results to the list\n",
        "    mann_whitney_results.append({\n",
        "        'Label': L,\n",
        "        'Statistic': statistic,\n",
        "        'P-Value': p_value,\n",
        "        'Significance': significance\n",
        "    })\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df_mann_whitney_results = pd.DataFrame(mann_whitney_results)\n",
        "\n",
        "\n",
        "\n",
        "# Save Mann-Whitney U test results to CSV with labels, statistics, p-values, and significance\n",
        "df_mann_whitney_results.to_csv('/content/drive/My Drive/ABIDE/min_80th_mann_whitney_results.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "s8WVHjpIi23H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical analysis part 3 (another non-parametric test)\n",
        "\n",
        "# Initialize an empty list for results\n",
        "wilcoxon_results = []\n",
        "\n",
        "label_list = ['min_DC', 'min_BC', 'min_CC', 'min_EC']\n",
        "\n",
        "for L in label_list:\n",
        "\n",
        "    list_control = []\n",
        "    list_patient = []\n",
        "\n",
        "    # Iterate over rows\n",
        "    for index, row in df.iterrows():\n",
        "        if row['Class'] == 'control':\n",
        "            list_control.append(row[L])\n",
        "        elif row['Class'] == 'patient':\n",
        "            list_patient.append(row[L])\n",
        "\n",
        "    # Perform Mann-Whitney U test\n",
        "    statistic, p_value = ranksums(list_control, list_patient)\n",
        "\n",
        "    # Interpret the results\n",
        "    significance = \"Significant\" if p_value < 0.05 else \"Not Significant\"\n",
        "\n",
        "    # Append results to the list\n",
        "    wilcoxon_results.append({\n",
        "        'Label': L,\n",
        "        'Statistic': statistic,\n",
        "        'P-Value': p_value,\n",
        "        'Significance': significance\n",
        "    })\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df_wilcoxon_results = pd.DataFrame(wilcoxon_results)\n",
        "\n",
        "# Save Mann-Whitney U test results to CSV with labels, statistics, p-values, and significance\n",
        "df_wilcoxon_results.to_csv('/content/drive/My Drive/ABIDE/wilcoxon_results_80th.csv', index=False)"
      ],
      "metadata": {
        "id": "hsEhlRmHM4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification step\n",
        "\n",
        "# K-NN implementation\n",
        "\n",
        "# Assuming 'Class' is the column with strings like 'control' or 'patient'\n",
        "label_encoder = LabelEncoder()\n",
        "df['Class'] = label_encoder.fit_transform(df['Class'])\n",
        "\n",
        "# Define Features and Target Variable\n",
        "X = df.iloc[:, 2]\n",
        "X = X.values.reshape(-1, 1)\n",
        "y = df['Class']\n",
        "\n",
        "# Split the dataset into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_k_value = 1\n",
        "\n",
        "# Initialize and Train the k-NN Classifier\n",
        "for k in range(1, 100):\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make Predictions\n",
        "    y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "    # Decode the numerical predictions back to original labels\n",
        "    y_pred_original_labels = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "    # Evaluate the Model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k_value = k\n",
        "        best_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a dictionary to store the metrics\n",
        "metrics_dict = {\n",
        "    'best k-value': best_k_value,\n",
        "    'accuracy': best_accuracy,\n",
        "    'confusion matrix': best_confusion_matrix\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "report_df = pd.DataFrame([metrics_dict])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "report_df.to_csv('/content/drive/My Drive/ABIDE/min_80th_classification_report.csv', index=False)"
      ],
      "metadata": {
        "id": "JjDdM-3-mr9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}